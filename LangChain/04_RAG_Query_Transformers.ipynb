{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55335fe4",
   "metadata": {},
   "source": [
    "# RAG: Query Tranformations\n",
    "\n",
    "Query transformations are a set of approaches focused on re-writing and/or modifying questions for retrieval.\n",
    "\n",
    "## Why Query Transformations Are Needed\n",
    "\n",
    "Raw user queries often don't match how information is stored in documents, leading to poor retrieval results.\n",
    "\n",
    "### Core Problems with Naive RAG\n",
    "\n",
    "*  **Ambiguous queries**: User questions can be vague or poorly worded for retrieval\n",
    "* **Semantic mismatch**: Query language differs from document language\n",
    "* **Limited perspective**: Single query may miss relevant documents\n",
    "* **Document chunks may contain irrelevant content** that degrades retrieval\n",
    "\n",
    "## How Query Transformation Solves these problems\n",
    "\n",
    "### Part 1: Multi-Query Translation\n",
    "\n",
    "* **Generates 3-5 query variations** using different phrasings and synonyms\n",
    "* **Retrieves documents for each variation** independently\n",
    "* **Combines unique results** from all queries\n",
    "* **Improves retrieval coverage** and increases likelihood of finding relevant documents\n",
    "\n",
    "### Part 2: RAG Fusion\n",
    "\n",
    "* **Uses Reciprocal Rank Fusion (RRF)** to rank and combine results from multiple query variations\n",
    "* **Provides better document ranking** than simple concatenation\n",
    "* **Handles duplicate results** across queries with improved scoring\n",
    "\n",
    "### Part 3: Decomposition\n",
    "\n",
    "* **Breaks complex questions into smaller sub-questions**\n",
    "* **Enables focused retrieval** for each sub-component\n",
    "* **Handles multi-faceted questions** more effectively\n",
    "* **Uses sequential or independent approaches** to answer sub-questions then combine\n",
    "\n",
    "### Part 4: Step-Back Prompting\n",
    "\n",
    "* **Creates broader, more general queries** alongside specific ones\n",
    "* **Captures high-level concepts** when specific queries are too narrow\n",
    "* **Combines both specific and general context** for comprehensive answers\n",
    "* **Provides broader understanding** for better reasoning\n",
    "\n",
    "### Part 5: HyDE (Hypothetical Document Embeddings)\n",
    "\n",
    "* **Generates hypothetical documents** instead of embedding queries directly\n",
    "* **Solves query-document semantic mismatch** in vector space\n",
    "* **Improves document-to-document similarity** matching\n",
    "* **Better alignment** between query representations and actual documents\n",
    "\n",
    "## Overall Impact\n",
    "* **Without Query Transformations**: Limited, potentially irrelevant results. \n",
    "* **With Query Transformations**: More comprehensive retrieval, better semantic matching, higher quality answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50bedf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (0.3.24)\n",
      "Requirement already satisfied: tiktoken in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (0.9.0)\n",
      "Requirement already satisfied: langchain-openai in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (0.3.18)\n",
      "Requirement already satisfied: langchainhub in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (0.1.21)\n",
      "Requirement already satisfied: chromadb in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (1.0.12)\n",
      "Requirement already satisfied: langchain in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (0.3.25)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.59 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from langchain_community) (0.3.63)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from langchain_community) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from langchain_community) (3.12.7)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from langchain_community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from langchain_community) (2.9.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from langchain_community) (0.3.43)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from langchain_community) (2.0.2)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from langchain) (2.11.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.59->langchain_community) (4.13.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
      "Requirement already satisfied: anyio in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
      "Requirement already satisfied: certifi in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from requests<3,>=2->langchain_community) (2.4.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from langchain-openai) (1.82.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.0)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from langchainhub) (2.32.0.20250602)\n",
      "Requirement already satisfied: build>=1.0.3 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: fastapi==0.115.9 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from chromadb) (0.115.9)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.3)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from chromadb) (4.2.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from chromadb) (1.19.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from chromadb) (1.33.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from chromadb) (1.33.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from chromadb) (0.54b1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from chromadb) (1.33.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from chromadb) (0.21.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from chromadb) (1.72.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from chromadb) (0.16.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from chromadb) (32.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from chromadb) (14.0.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from chromadb) (4.24.0)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from fastapi==0.115.9->chromadb) (0.45.3)\n",
      "Requirement already satisfied: pyproject_hooks in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from build>=1.0.3->chromadb) (8.6.1)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from build>=1.0.3->chromadb) (2.2.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from importlib-metadata>=4.6->build>=1.0.3->chromadb) (3.22.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from jsonschema>=4.19.0->chromadb) (0.25.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (2.40.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Requirement already satisfied: coloredlogs in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
      "Requirement already satisfied: sympy in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb) (1.17.2)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.33.1 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.33.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.33.1 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.33.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.54b1 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.54b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.54b1 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.54b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.54b1 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.54b1)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.54b1 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.54b1)\n",
      "Requirement already satisfied: asgiref~=3.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from opentelemetry-instrumentation-asgi==0.54b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from tokenizers>=0.13.2->chromadb) (0.32.4)\n",
      "Requirement already satisfied: filelock in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.5.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\n",
      "Requirement already satisfied: websockets>=10.4 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/.venv/lib/python3.9/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd3a9113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    " \n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4e2f3b",
   "metadata": {},
   "source": [
    "## Part 1 - Multi Query Translation\n",
    "\n",
    "<img src=\"multi-query-translation.png\" width=\"800\" height=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d866566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# Index\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cc405b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | ChatOpenAI(model_name=\"gpt-4o-mini\",temperature=0) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eaaccd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_58767/911277764.py:10: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  return [loads(doc) for doc in unique_docs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union # removes duplicate documents\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be8d3edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2461579b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM (large language model) agents involves breaking down complex tasks into smaller, manageable subgoals. This process enables the agent to handle intricate tasks more efficiently. There are several methods for task decomposition:\\n\\n1. **Chain of Thought (CoT)**: This technique encourages the model to \"think step by step,\" allowing it to utilize more computational resources at test time to simplify hard tasks into smaller, more manageable steps.\\n\\n2. **Tree of Thoughts**: This method extends CoT by exploring multiple reasoning possibilities at each step. It decomposes the problem into various thought steps and generates multiple thoughts for each step, creating a tree structure. The search process can be conducted using breadth-first search (BFS) or depth-first search (DFS), with each state evaluated by a classifier or through majority voting.\\n\\n3. **Prompting and Instructions**: Task decomposition can also be achieved through simple prompting (e.g., asking the model for steps or subgoals), using task-specific instructions (e.g., \"Write a story outline\" for writing tasks), or incorporating human inputs.\\n\\nOverall, task decomposition is a crucial component that enhances the planning capabilities of LLM-powered autonomous agents.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9e8598",
   "metadata": {},
   "source": [
    "## Part 2 - RAG Fusion\n",
    "\n",
    "<img src=\"rag-fusion.png\" width=\"800\" height=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e396069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG-Fusion: Related\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24af7d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3e87553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2444858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory'),\n",
       "  0.13118237599993285),\n",
       " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
       "  0.06506215742069787),\n",
       " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.'),\n",
       "  0.06400409626216078)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d4027fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM (large language model) agents involves breaking down complex tasks into smaller, manageable subgoals. This process enables the agent to handle intricate tasks more efficiently. Techniques such as Chain of Thought (CoT) and Tree of Thoughts are commonly used for task decomposition. \\n\\n- **Chain of Thought (CoT)** encourages the model to \"think step by step,\" allowing it to decompose difficult tasks into simpler steps, which also provides insight into the model\\'s reasoning process.\\n- **Tree of Thoughts** extends this concept by exploring multiple reasoning possibilities at each step, creating a tree structure where the problem is decomposed into various thought steps, and multiple thoughts are generated for each step.\\n\\nTask decomposition can be achieved through simple prompting, task-specific instructions, or human inputs, facilitating a structured approach to problem-solving for LLM agents.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "134563eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "  1. 1. How does task decomposition work in large language model (LLM) agents?\n",
      "  2. 2. Benefits of task decomposition for LLM agents in AI applications.\n",
      "  3. 3. Examples of task decomposition techniques used in LLM agents.\n",
      "  4. 4. Comparing task decomposition methods for improving LLM agent performance.\n",
      "\n",
      "Retrieving documents for each query:\n",
      "\n",
      "Query 1: '1. How does task decomposition work in large language model (LLM) agents?'\n",
      "  Retrieved 4 documents\n",
      "    Doc 1: LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author...\n",
      "    Doc 2: LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author...\n",
      "\n",
      "Query 2: '2. Benefits of task decomposition for LLM agents in AI applications.'\n",
      "  Retrieved 4 documents\n",
      "    Doc 1: LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author...\n",
      "    Doc 2: LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author...\n",
      "\n",
      "Query 3: '3. Examples of task decomposition techniques used in LLM agents.'\n",
      "  Retrieved 4 documents\n",
      "    Doc 1: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what...\n",
      "    Doc 2: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what...\n",
      "\n",
      "Query 4: '4. Comparing task decomposition methods for improving LLM agent performance.'\n",
      "  Retrieved 4 documents\n",
      "    Doc 1: LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author...\n",
      "    Doc 2: LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author...\n",
      "\n",
      "After RRF: 3 unique documents\n",
      "  Rank 1 (score: 0.1312): LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author...\n",
      "  Rank 2 (score: 0.0651): Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what...\n",
      "  Rank 3 (score: 0.0640): Finite context length: The restricted context capacity limits the inclusion of historical informatio...\n",
      "\n",
      "Context length: 3544 characters\n"
     ]
    }
   ],
   "source": [
    "# Instead of running the full chain, run each step separately\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "\n",
    "# Step 1: Generate multiple queries\n",
    "generate_queries_chain = (\n",
    "    ChatPromptTemplate.from_template(\n",
    "        'You are a helpful assistant that generates multiple search queries based on a single input query. \\n\\nGenerate multiple search queries related to: {question} \\n\\nOutput (4 queries):'\n",
    "    ) \n",
    "    | ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "queries = generate_queries_chain.invoke({\"question\": question})\n",
    "print(\"Generated queries:\")\n",
    "for i, q in enumerate(queries):\n",
    "    print(f\"  {i+1}. {q}\")\n",
    "\n",
    "# Step 2: Retrieve documents for each query\n",
    "print(\"\\nRetrieving documents for each query:\")\n",
    "all_results = []\n",
    "for i, query in enumerate(queries):\n",
    "    docs = retriever.invoke(query)\n",
    "    print(f\"\\nQuery {i+1}: '{query}'\")\n",
    "    print(f\"  Retrieved {len(docs)} documents\")\n",
    "    for j, doc in enumerate(docs[:2]):  # Show first 2 docs\n",
    "        print(f\"    Doc {j+1}: {doc.page_content[:100]}...\")\n",
    "    all_results.append(docs)\n",
    "\n",
    "# Step 3: Apply RRF and get unique union\n",
    "fused_results = reciprocal_rank_fusion(all_results)\n",
    "print(f\"\\nAfter RRF: {len(fused_results)} unique documents\")\n",
    "for i, (doc, score) in enumerate(fused_results[:3]):\n",
    "    print(f\"  Rank {i+1} (score: {score:.4f}): {doc.page_content[:100]}...\")\n",
    "\n",
    "# Step 4: Final answer generation\n",
    "context = \"\\n\\n\".join([doc.page_content for doc, _ in fused_results[:5]])\n",
    "final_prompt = f\"Answer the following question based on this context:\\n\\n{context}\\n\\nQuestion: {question}\"\n",
    "print(f\"\\nContext length: {len(context)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a5a5b1",
   "metadata": {},
   "source": [
    "## Part 3 - Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1d9eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Decomposition\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d7677dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "# Run\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4053f69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What are the key components of a large language model (LLM) used in autonomous agent systems?',\n",
       " '2. How do perception and decision-making modules integrate with LLMs in autonomous agents?',\n",
       " '3. What role does reinforcement learning play in enhancing LLM-powered autonomous agent systems?']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ec628",
   "metadata": {},
   "source": [
    "**Answer Recursively**\n",
    "\n",
    "<img src=\"answers-recursively.png\" width=\"800\" height=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d70da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba9fcfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "# llm\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    \n",
    "    rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \n",
    "     \"question\": itemgetter(\"question\"),\n",
    "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n",
    "    | decomposition_prompt\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q,answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b379eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n---\\nQuestion: 1. What are the key components of a large language model (LLM) used in autonomous agent systems?\\nAnswer: The key components of a large language model (LLM) used in autonomous agent systems include:\\n\\n1. **Planning**: This involves the ability of the agent to break down large tasks into smaller, manageable subgoals, which facilitates efficient handling of complex tasks.\\n\\n2. **Subgoal and Decomposition**: The agent can decompose tasks into subgoals, allowing for a structured approach to problem-solving.\\n\\n3. **Reflection and Refinement**: The agent is capable of self-criticism and self-reflection over past actions, enabling it to learn from mistakes and refine its approach for future tasks, thereby improving the quality of its results.\\n\\n4. **Memory**: The system may incorporate memory components to retain information over time, although challenges exist due to the finite context length of LLMs, which limits the inclusion of historical information and detailed instructions.\\n\\n5. **Natural Language Interface**: The agent relies on natural language as an interface to communicate with external components, such as memory and tools, although the reliability of this interface can be variable.\\n\\n6. **Challenges in Long-term Planning**: LLMs face difficulties in long-term planning and task decomposition, particularly when adjusting plans in response to unexpected errors.\\n\\nThese components work together to enable LLMs to function effectively as the core controller in autonomous agent systems.\\n---\\nQuestion: 2. How do perception and decision-making modules integrate with LLMs in autonomous agents?\\nAnswer: In autonomous agents powered by large language models (LLMs), perception and decision-making modules play crucial roles in enabling the agent to interact effectively with its environment and make informed choices. Here's how these modules integrate with LLMs:\\n\\n1. **Perception**: The perception module is responsible for gathering and interpreting data from the environment. This can include sensory inputs such as visual data, audio signals, or other forms of information. The LLM can process this data by translating it into a format that it can understand, often through natural language descriptions. For instance, if the perception module detects an object, it can relay this information to the LLM, which can then analyze and contextualize it within the task at hand.\\n\\n2. **Decision-Making**: The decision-making module utilizes the insights gained from the perception module to make choices about the agent's actions. This involves planning and reasoning, where the LLM can generate potential action plans based on the perceived data. The integration of decision-making with LLMs allows for a more nuanced approach to problem-solving, as the LLM can leverage its ability to reflect on past actions and refine its strategies. For example, using self-reflection, the LLM can evaluate the outcomes of previous decisions and adjust its future actions accordingly.\\n\\n3. **ReAct Framework**: The ReAct framework exemplifies the integration of reasoning and acting within LLMs. It expands the action space to include both discrete actions (specific tasks) and language-based reasoning. This allows the LLM to not only decide on actions based on perceptions but also articulate its reasoning process, which can enhance transparency and adaptability in decision-making.\\n\\n4. **External Planning Tools**: In some cases, LLMs may work in conjunction with external classical planners for long-term decision-making. This approach allows the LLM to focus on interpreting the environment and generating immediate responses while delegating complex planning tasks to specialized tools. The LLM can translate the planning problem into a format suitable for the planner and then interpret the planner's output back into actionable language.\\n\\nOverall, the integration of perception and decision-making modules with LLMs in autonomous agents creates a dynamic system capable of understanding its environment, making informed decisions, and learning from experiences, thereby enhancing its effectiveness in real-world tasks.\\n---\\nQuestion: 3. What role does reinforcement learning play in enhancing LLM-powered autonomous agent systems?\\nAnswer: Reinforcement learning (RL) plays a significant role in enhancing LLM-powered autonomous agent systems by providing a framework for these agents to learn from interactions with their environment and improve their decision-making capabilities over time. Here are several key aspects of how RL contributes to these systems:\\n\\n1. **Learning from Feedback**: In RL, agents learn by receiving feedback in the form of rewards or penalties based on their actions. This feedback loop allows LLM-powered agents to evaluate the effectiveness of their decisions and refine their strategies accordingly. By incorporating RL, agents can optimize their performance on tasks by understanding which actions lead to better outcomes.\\n\\n2. **Exploration and Exploitation**: RL encourages a balance between exploration (trying new actions to discover their effects) and exploitation (choosing the best-known actions based on past experiences). This dynamic is crucial for LLM-powered agents, as it enables them to adapt to changing environments and improve their problem-solving capabilities by exploring various strategies.\\n\\n3. **Long-term Planning**: While LLMs can struggle with long-term planning and task decomposition, RL can enhance this capability by allowing agents to evaluate the long-term consequences of their actions. By simulating different scenarios and outcomes, RL helps agents make more informed decisions that consider future states, thereby improving their overall effectiveness in complex tasks.\\n\\n4. **Integration with Decision-Making Modules**: RL can be integrated with the decision-making modules of LLM-powered agents, allowing them to leverage the strengths of both approaches. The LLM can generate potential action plans based on perceived data, while the RL component can assess these plans based on learned experiences, leading to more robust and adaptive decision-making.\\n\\n5. **Self-Reflection and Refinement**: Reinforcement learning complements the self-reflection and refinement capabilities of LLMs. As agents learn from their past actions and outcomes, they can adjust their strategies and improve their performance over time. This iterative learning process is essential for developing agents that can handle complex, real-world tasks effectively.\\n\\n6. **Tool Use and External APIs**: RL can also enhance the agent's ability to utilize external tools and APIs effectively. By learning which tools to use in specific contexts and how to integrate them into their decision-making processes, LLM-powered agents can expand their capabilities and access information beyond their pre-trained knowledge.\\n\\nIn summary, reinforcement learning enhances LLM-powered autonomous agent systems by providing a structured approach to learning from interactions, optimizing decision-making, and enabling long-term planning. This synergy allows agents to become more adaptive, efficient, and capable of handling complex tasks in dynamic environments.\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_a_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1bfe55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Reinforcement learning (RL) plays a significant role in enhancing LLM-powered autonomous agent systems by providing a framework for these agents to learn from interactions with their environment and improve their decision-making capabilities over time. Here are several key aspects of how RL contributes to these systems:\\n\\n1. **Learning from Feedback**: In RL, agents learn by receiving feedback in the form of rewards or penalties based on their actions. This feedback loop allows LLM-powered agents to evaluate the effectiveness of their decisions and refine their strategies accordingly. By incorporating RL, agents can optimize their performance on tasks by understanding which actions lead to better outcomes.\\n\\n2. **Exploration and Exploitation**: RL encourages a balance between exploration (trying new actions to discover their effects) and exploitation (choosing the best-known actions based on past experiences). This dynamic is crucial for LLM-powered agents, as it enables them to adapt to changing environments and improve their problem-solving capabilities by exploring various strategies.\\n\\n3. **Long-term Planning**: While LLMs can struggle with long-term planning and task decomposition, RL can enhance this capability by allowing agents to evaluate the long-term consequences of their actions. By simulating different scenarios and outcomes, RL helps agents make more informed decisions that consider future states, thereby improving their overall effectiveness in complex tasks.\\n\\n4. **Integration with Decision-Making Modules**: RL can be integrated with the decision-making modules of LLM-powered agents, allowing them to leverage the strengths of both approaches. The LLM can generate potential action plans based on perceived data, while the RL component can assess these plans based on learned experiences, leading to more robust and adaptive decision-making.\\n\\n5. **Self-Reflection and Refinement**: Reinforcement learning complements the self-reflection and refinement capabilities of LLMs. As agents learn from their past actions and outcomes, they can adjust their strategies and improve their performance over time. This iterative learning process is essential for developing agents that can handle complex, real-world tasks effectively.\\n\\n6. **Tool Use and External APIs**: RL can also enhance the agent's ability to utilize external tools and APIs effectively. By learning which tools to use in specific contexts and how to integrate them into their decision-making processes, LLM-powered agents can expand their capabilities and access information beyond their pre-trained knowledge.\\n\\nIn summary, reinforcement learning enhances LLM-powered autonomous agent systems by providing a structured approach to learning from interactions, optimizing decision-making, and enabling long-term planning. This synergy allows agents to become more adaptive, efficient, and capable of handling complex tasks in dynamic environments.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61db47a0",
   "metadata": {},
   "source": [
    "**Answer Individually**\n",
    "\n",
    "<img src=\"answer_individually.png\" width=\"800\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b62f1509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_58767/3122629327.py:24: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(sub_question)\n"
     ]
    }
   ],
   "source": [
    "# Answer each sub-question individually \n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# RAG prompt\n",
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def retrieve_and_rag(question,prompt_rag,sub_question_generator_chain):\n",
    "    \"\"\"RAG on each sub-question\"\"\"\n",
    "    \n",
    "    # Use our decomposition / \n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
    "    \n",
    "    # Initialize a list to hold RAG chain results\n",
    "    rag_results = []\n",
    "    \n",
    "    for sub_question in sub_questions:\n",
    "        \n",
    "        # Retrieve documents for each sub-question\n",
    "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "        \n",
    "        # Use retrieved documents and sub-question in RAG chain\n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs, \n",
    "                                                                \"question\": sub_question})\n",
    "        rag_results.append(answer)\n",
    "    \n",
    "    return rag_results,sub_questions\n",
    "\n",
    "# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
    "answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb28002c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECOMPOSED QUESTIONS AND ANSWERS\n",
      "\n",
      "Q1: 1. What are the key components of a large language model (LLM) used in autonomous agent systems?\n",
      "A1: The key components of a large language model (LLM) used in autonomous agent systems include planning, subgoal decomposition, and reflection and refinement. These components enable the agent to break down complex tasks into manageable parts, learn from past actions, and improve future performance. Additionally, memory plays a crucial role in enhancing the agent's capabilities.\n",
      "\n",
      "Q2: 2. How do perception and decision-making modules integrate with LLMs in autonomous agents?\n",
      "A2: Perception and decision-making modules in LLM-powered autonomous agents integrate by allowing the LLM to function as the agent's brain, facilitating planning and self-reflection. The agent can break down complex tasks into manageable subgoals and learn from past actions to improve future decisions. Additionally, approaches like ReAct combine reasoning and acting, enabling the LLM to interact with the environment while generating reasoning traces.\n",
      "\n",
      "Q3: 3. What role does reinforcement learning play in enhancing LLM-powered autonomous agent systems?\n",
      "A3: Reinforcement learning enhances LLM-powered autonomous agent systems by enabling agents to learn from their actions through self-criticism and reflection. This process allows agents to refine their strategies and improve their performance over time. Additionally, it supports the breakdown of complex tasks into manageable subgoals, facilitating more efficient problem-solving.\n"
     ]
    }
   ],
   "source": [
    "print(\"DECOMPOSED QUESTIONS AND ANSWERS\")\n",
    "\n",
    "for i, (q, a) in enumerate(zip(questions, answers), 1):\n",
    "    print(f\"\\nQ{i}: {q}\")\n",
    "    print(f\"A{i}: {a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c9f5bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"An LLM-powered autonomous agent system comprises several main components that work together to enhance its functionality and performance. Key components include:\\n\\n1. **Large Language Model (LLM)**: The LLM serves as the core processing unit, functioning as the agent's brain. It facilitates planning, self-reflection, and the decomposition of complex tasks into manageable subgoals.\\n\\n2. **Perception Module**: This module allows the agent to perceive and interpret its environment, providing the necessary input for decision-making processes.\\n\\n3. **Decision-Making Module**: Integrated with the LLM, this module enables the agent to make informed decisions based on its perceptions and learned experiences.\\n\\n4. **Reinforcement Learning**: This component enhances the agent's ability to learn from its actions through self-criticism and reflection, allowing it to refine strategies and improve performance over time.\\n\\n5. **Memory**: Memory plays a crucial role in storing past experiences and learned strategies, which helps the agent to improve future decision-making and problem-solving capabilities.\\n\\n6. **ReAct Approach**: This approach combines reasoning and acting, enabling the LLM to interact with the environment while generating reasoning traces, further enhancing the agent's ability to navigate complex tasks.\\n\\nTogether, these components enable LLM-powered autonomous agents to effectively plan, learn, and adapt in dynamic environments.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\":context,\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405cdb60",
   "metadata": {},
   "source": [
    "## Part 4 - Step Back "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58481b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Examples\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\", #more abstract\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "# We now transform these to example messages\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        # Few shot examples\n",
    "        few_shot_prompt,\n",
    "        # New question\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9298a464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is the process of breaking down tasks for agents?'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries_step_back = prompt | ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0) | StrOutputParser()\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_queries_step_back.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3260721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM (large language model) agents refers to the process of breaking down complex tasks into smaller, more manageable subgoals or steps. This approach is essential for enabling LLM agents to handle intricate tasks efficiently and effectively. \\n\\nThere are several techniques and methodologies for task decomposition:\\n\\n1. **Chain of Thought (CoT)**: This technique encourages the model to \"think step by step,\" allowing it to utilize more computational resources during test time. By decomposing a complex task into simpler, smaller tasks, CoT not only enhances the model\\'s performance but also provides insights into its reasoning process.\\n\\n2. **Tree of Thoughts**: An extension of CoT, this method explores multiple reasoning possibilities at each step. It decomposes the problem into various thought steps and generates multiple thoughts for each step, forming a tree structure. The search process can be conducted using either breadth-first search (BFS) or depth-first search (DFS), with each state evaluated by a classifier or through majority voting.\\n\\n3. **Prompting Techniques**: Task decomposition can be initiated through simple prompts directed at the LLM, such as asking for the steps to achieve a specific goal or identifying subgoals. For example, prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" can guide the model in breaking down tasks.\\n\\n4. **Task-Specific Instructions**: Providing specific instructions tailored to the task at hand can also facilitate decomposition. For instance, asking the model to \"Write a story outline\" for a novel can help it focus on the necessary components of that task.\\n\\n5. **Human Inputs**: In some cases, human guidance can be used to assist the LLM in identifying and structuring subgoals, ensuring that the decomposition aligns with the desired outcomes.\\n\\nOverall, task decomposition is a critical component of planning in LLM-powered autonomous agents, allowing them to navigate complex tasks more effectively by structuring their approach into manageable parts.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "# Response prompt \n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        # Retrieve context using the normal question\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        # Retrieve context using the step-back question\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        # Pass on the question\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d3d9212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL QUESTION: What is task decomposition for LLM agents?\n",
      "================================================================================\n",
      "\n",
      "GENERATING STEP-BACK QUERY\n",
      "Step-back query: what does task decomposition mean in the context of agents?\n",
      "\n",
      "RETRIEVING WITH NORMAL QUESTION\n",
      "Normal retrieval: 4 documents\n",
      "Normal context preview: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a s...\n",
      "\n",
      "RETRIEVING WITH STEP-BACK QUESTION\n",
      "Step-back retrieval: 4 documents\n",
      "Step-back context preview: Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a s...\n",
      "\n",
      "BUILDING RESPONSE PROMPT\n",
      "Final prompt length: 9786 characters\n",
      "Final prompt preview: You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
      "\n",
      "# Component One: Planning#\n",
      "A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n",
      "Task Decomposition#\n",
      "Chain of thought (CoT;...\n",
      "\n",
      "GENERATING FINAL ANSWER\n",
      "Final answer: Task decomposition for LLM (large language model) agents refers to the process of breaking down complex tasks into smaller, more manageable subgoals or steps. This approach enhances the efficiency and effectiveness of the agent in handling intricate problems. Here are the key aspects of task decomposition for LLM agents:\n",
      "\n",
      "1. **Purpose**: The primary goal of task decomposition is to simplify complex tasks, making them easier to understand and execute. By dividing a large task into smaller components, the agent can focus on one aspect at a time, which reduces cognitive load and increases the likelihood of successful completion.\n",
      "\n",
      "2. **Techniques**:\n",
      "   - **Chain of Thought (CoT)**: This technique encourages the model to \"think step by step,\" allowing it to utilize more computational resources to break down difficult tasks into simpler steps. CoT not only aids in task execution but also provides insights into the model's reasoning process.\n",
      "   - **Tree of Thoughts**: An extension of CoT, this method explores multiple reasoning possibilities at each step. It generates a tree structure of thoughts, where each node represents a potential thought or action. The search process can be conducted using breadth-first search (BFS) or depth-first search (DFS), with each state evaluated to determine the best path forward.\n",
      "\n",
      "3. **Implementation**: Task decomposition can be achieved through various means:\n",
      "   - **Prompting**: Simple prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" can guide the LLM in identifying the necessary steps.\n",
      "   - **Task-Specific Instructions**: Providing specific instructions tailored to the task, such as \"Write a story outline,\" can help the model understand the context and requirements better.\n",
      "   - **Human Inputs**: Human guidance can also play a role in defining subgoals and steps, especially in complex scenarios where human intuition and experience are beneficial.\n",
      "\n",
      "4. **Benefits**: Effective task decomposition allows LLM agents to manage complexity, improve their problem-solving capabilities, and enhance the quality of their outputs. It also facilitates self-reflection and refinement, as agents can learn from past actions and adjust their approaches accordingly.\n",
      "\n",
      "5. **Challenges**: Despite its advantages, task decomposition faces challenges such as the limited context length of LLMs, which restricts the amount of historical information and detailed instructions that can be processed. Additionally, LLMs may struggle with long-term planning and adapting to unexpected errors, making them less robust compared to human problem solvers.\n",
      "\n",
      "In summary, task decomposition is a critical component of LLM-powered autonomous agents, enabling them to tackle complex tasks more effectively by breaking them down into smaller, manageable parts.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is task decomposition for LLM agents?\"\n",
    "input_data = {\"question\": question}\n",
    "\n",
    "print(f\"ORIGINAL QUESTION: {question}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Step 1: Generate step-back query\n",
    "print(\"\\nGENERATING STEP-BACK QUERY\")\n",
    "step_back_query = generate_queries_step_back.invoke(input_data)\n",
    "print(f\"Step-back query: {step_back_query}\")\n",
    "\n",
    "# Step 2: Retrieve context using normal question\n",
    "print(\"\\nRETRIEVING WITH NORMAL QUESTION\")\n",
    "normal_docs = retriever.invoke(question)\n",
    "normal_context = \"\\n\\n\".join([doc.page_content for doc in normal_docs])\n",
    "print(f\"Normal retrieval: {len(normal_docs)} documents\")\n",
    "print(f\"Normal context preview: {normal_context[:200]}...\")\n",
    "\n",
    "# Step 3: Retrieve context using step-back question\n",
    "print(\"\\nRETRIEVING WITH STEP-BACK QUESTION\")\n",
    "step_back_docs = retriever.invoke(step_back_query)\n",
    "step_back_context = \"\\n\\n\".join([doc.page_content for doc in step_back_docs])\n",
    "print(f\"Step-back retrieval: {len(step_back_docs)} documents\")\n",
    "print(f\"Step-back context preview: {step_back_context[:200]}...\")\n",
    "\n",
    "# Step 4: Build final prompt\n",
    "print(\"\\nBUILDING RESPONSE PROMPT\")\n",
    "prompt_inputs = {\n",
    "    \"normal_context\": normal_context,\n",
    "    \"step_back_context\": step_back_context,\n",
    "    \"question\": question\n",
    "}\n",
    "formatted_prompt = response_prompt.invoke(prompt_inputs)\n",
    "print(f\"Final prompt length: {len(formatted_prompt.messages[0].content)} characters\")\n",
    "print(f\"Final prompt preview: {formatted_prompt.messages[0].content[:400]}...\")\n",
    "\n",
    "# Step 5: Get final answer\n",
    "print(\"\\nGENERATING FINAL ANSWER\")\n",
    "final_answer = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0).invoke(formatted_prompt)\n",
    "print(f\"Final answer: {final_answer.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7526401c",
   "metadata": {},
   "source": [
    "## Part 5 - HyDE (Hypothetical Document Embedding)\n",
    "\n",
    "<img src=\"HyDE.png\" width=\"800\" height=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d3a92cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Task Decomposition for LLM Agents**\\n\\nTask decomposition is a critical strategy employed in the development and application of Large Language Model (LLM) agents, facilitating the breakdown of complex tasks into manageable sub-tasks. This approach enhances the efficiency and effectiveness of LLMs in various applications, including natural language understanding, dialogue systems, and automated reasoning.\\n\\nAt its core, task decomposition involves identifying the constituent components of a larger task and systematically addressing each component. For LLM agents, this process can be categorized into several stages: task analysis, sub-task identification, and execution strategy formulation. During task analysis, the primary objective is to understand the overarching goal and the specific requirements necessary for its completion. This may involve parsing the task into distinct phases, such as information retrieval, data processing, and response generation.\\n\\nOnce the task is analyzed, the next step is sub-task identification, where the LLM agent delineates the individual tasks that contribute to the overall objective. For instance, in a question-answering scenario, sub-tasks may include interpreting the question, searching for relevant information, synthesizing the findings, and formulating a coherent response. Each of these sub-tasks can be further refined, allowing the LLM to leverage its capabilities more effectively.\\n\\nThe final stage, execution strategy formulation, involves determining the optimal sequence and method for addressing the identified sub-tasks. This may include prioritizing certain tasks based on their dependencies or employing parallel processing techniques to enhance throughput. By adopting a structured approach to task decomposition, LLM agents can improve their performance, reduce error rates, and provide more accurate and contextually relevant outputs.\\n\\nIn summary, task decomposition for LLM agents is a systematic approach that enhances their ability to tackle complex tasks by breaking them down into smaller, more manageable components. This methodology not only streamlines the processing capabilities of LLMs but also aligns their operations with human cognitive strategies, ultimately leading to more sophisticated and reliable AI systems.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# HyDE document genration\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde | ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0) | StrOutputParser() \n",
    ")\n",
    "\n",
    "# Run\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_docs_for_retrieval.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "874f3075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve\n",
    "retrieval_chain = generate_docs_for_retrieval | retriever \n",
    "retireved_docs = retrieval_chain.invoke({\"question\":question})\n",
    "retireved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3df2d48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM (large language model) agents involves breaking down complex tasks into smaller, manageable subgoals. This process enables the agent to handle complicated tasks more efficiently. There are several methods for task decomposition:\\n\\n1. **Chain of Thought (CoT)**: This technique encourages the model to \"think step by step,\" allowing it to utilize more computational resources at test time to decompose difficult tasks into simpler steps. CoT transforms large tasks into multiple manageable tasks and provides insight into the model\\'s reasoning process.\\n\\n2. **Tree of Thoughts**: This method extends CoT by exploring multiple reasoning possibilities at each step. It decomposes the problem into various thought steps and generates multiple thoughts for each step, creating a tree structure. The search process can be conducted using breadth-first search (BFS) or depth-first search (DFS), with each state evaluated by a classifier or through majority voting.\\n\\n3. **Prompting and Instructions**: Task decomposition can also be achieved through simple prompting, such as asking for the steps to achieve a goal or identifying subgoals. Additionally, task-specific instructions can guide the agent, like requesting a story outline for writing a novel.\\n\\n4. **Human Inputs**: Human guidance can also play a role in task decomposition, providing insights and direction for the agent.\\n\\nOverall, task decomposition is a critical component of planning in LLM-powered autonomous agents, allowing them to approach complex problems systematically.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\":retireved_docs,\"question\":question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
