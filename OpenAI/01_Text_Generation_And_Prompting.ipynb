{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c8d8266",
   "metadata": {},
   "source": [
    "Text generation and prompting\n",
    "=============================\n",
    "\n",
    "Learn how to prompt a model to generate text.\n",
    "\n",
    "With the OpenAI API, you can use a [large language model](/docs/models) to generate text from a prompt, as you might using [ChatGPT](https://chatgpt.com). Models can generate almost any kind of text response‚Äîlike code, mathematical equations, structured JSON data, or human-like prose.\n",
    "\n",
    "Here's a simple example using the [Responses API](/docs/api-reference/responses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c320111d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/thann/Library/CloudStorage/OneDrive-IndianInstituteofScience/Course_Contents/Deep Learning/Class Materials/Tutorials/deep-learning/OpenAI\n",
      "API key found: sk-p...qb0A\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Print the current working directory to verify where Python is looking for the .env file\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Print the API key (first few characters for security)\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if api_key:\n",
    "    print(\"API key found:\", api_key[:4] + \"...\" + api_key[-4:])\n",
    "else:\n",
    "    print(\"No API key found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db911cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As the moonlight shimmered on the enchanted forest, a lonely unicorn named Luna discovered a hidden glade where her radiant horn brought flowers to bloom, filling the night with magic and joy.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b540e00e",
   "metadata": {},
   "source": [
    "**Note**: An array of content generated by the model is in the `output` property of the response. In this simple example, we have just one output which looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e8e625",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    {\n",
    "        \"id\": \"msg_67b73f697ba4819183a15cc17d011509\",\n",
    "        \"type\": \"message\",\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"output_text\",\n",
    "                \"text\": \"Under the soft glow of the moon, Luna the unicorn danced through fields of twinkling stardust, leaving trails of dreams for every child asleep.\",\n",
    "                \"annotations\": []\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52231629",
   "metadata": {},
   "source": [
    "**The `output` array often has more than one item in it!** It can contain tool calls, data about reasoning tokens generated by [reasoning models](/docs/guides/reasoning), and other items. It is not safe to assume that the model's text output is present at `output[0].content[0].text`.\n",
    "\n",
    "Some of our [official SDKs](/docs/libraries) include an `output_text` property on model responses for convenience, which aggregates all text outputs from the model into a single string. This may be useful as a shortcut to access text output from the model.\n",
    "\n",
    "In addition to plain text, you can also have the model return structured data in JSON format - this feature is called [**Structured Outputs**](/docs/guides/structured-outputs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8929f0aa",
   "metadata": {},
   "source": [
    "Choosing a model\n",
    "----------------\n",
    "\n",
    "A key choice to make when generating content through the API is which model you want to use - the `model` parameter of the code samples above. [You can find a full listing of available models here](/docs/models). Here are a few factors to consider when choosing a model for text generation.\n",
    "\n",
    "*   **[Reasoning models](/docs/guides/reasoning)** generate an internal chain of thought to analyze the input prompt, and excel at understanding complex tasks and multi-step planning. They are also generally slower and more expensive to use than GPT models.\n",
    "*   **GPT models** are fast, cost-efficient, and highly intelligent, but benefit from more explicit instructions around how to accomplish tasks.\n",
    "*   **Large and small (mini or nano) models** offer trade-offs for speed, cost, and intelligence. Large models are more effective at understanding prompts and solving problems across domains, while small models are generally faster and cheaper to use.\n",
    "\n",
    "When in doubt, [`gpt-4.1`](/docs/models/gpt-4.1) offers a solid combination of intelligence, speed, and cost effectiveness.\n",
    "\n",
    "Prompt engineering\n",
    "------------------\n",
    "\n",
    "**Prompt engineering** is the process of writing effective instructions for a model, such that it consistently generates content that meets your requirements.\n",
    "\n",
    "Because the content generated from a model is non-deterministic, it is a combination of art and science to build a prompt that will generate content in the format you want. However, there are a number of techniques and best practices you can apply to consistently get good results from a model.\n",
    "\n",
    "Some prompt engineering techniques will work with every model, like using message roles. But different model types (like reasoning versus GPT models) might need to be prompted differently to produce the best results. Even different snapshots of models within the same family could produce different results. So as you are building more complex applications, we strongly recommend that you:\n",
    "\n",
    "*   **Pin your production applications to specific [model snapshots](/docs/models) (like `gpt-4.1-2025-04-14` for example) to ensure consistent behavior**.\n",
    "*   **Build [evals](/docs/guides/evals)** that will measure the behavior of your prompts, so that you can monitor the performance of your prompts as you iterate on them, or when you change and upgrade model versions.\n",
    "\n",
    "Now, let's examine some tools and techniques available to you to construct prompts.\n",
    "\n",
    "Message roles and instruction following\n",
    "---------------------------------------\n",
    "\n",
    "You can provide instructions to the model with [differing levels of authority](https://model-spec.openai.com/2025-02-12.html#chain_of_command) using the `instructions` API parameter or **message roles**.\n",
    "\n",
    "The `instructions` parameter gives the model high-level instructions on how it should behave while generating a response, including tone, goals, and examples of correct responses. **Any instructions provided this way will take priority over a prompt in the `input` parameter.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7783ca50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aye, matey! In the seas of JavaScript, semicolons be mostly optional due to the mighty feature known as Automatic Semicolon Insertion (ASI). But beware! Relyin' solely on ASI can lead to treacherous waters and unexpected behaviors. It be good practice to use semicolons to ensure yer code be clearer and free o' errors. So sail wisely, me heartie, and mark yer ends with semicolons when ye can! Arrr! üè¥‚Äç‚ò†Ô∏è\n"
     ]
    }
   ],
   "source": [
    "# Generate Text with Instructions\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    instructions=\"Talk like a pirate.\",\n",
    "    input=\"Are semicolons optional in JavaScript?\",\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b408f5b",
   "metadata": {},
   "source": [
    "The example above is roughly equivalent to using the following input messages in the `input` array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54d84da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, matey! In the treacherous waters of JavaScript, semicolons be oft considered optional due to a clever trick known as Automatic Semicolon Insertion (ASI). But beware! While ye can sail without 'em, 'tis wise to keep 'em onboard to avoid perilous pitfalls and unexpected behaviors in yer code. So, if ye want smooth sailing, I say, hoist the semicolons! ‚öìÔ∏èüíª\n"
     ]
    }
   ],
   "source": [
    "# Generate text with messages using different roles\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"Talk like a pirate.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Are semicolons optional in JavaScript?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44527864",
   "metadata": {},
   "source": [
    "Note that the `instructions` parameter only applies to the current response generation request. If you are [managing conversation state](/docs/guides/conversation-state) with the `previous_response_id` parameter, the `instructions` used on previous turns will not be present in the context.\n",
    "\n",
    "The [OpenAI model spec](https://model-spec.openai.com/2025-02-12.html#chain_of_command) describes how our models give different levels of priority to messages with different roles.\n",
    "\n",
    "|developer|user|assistant|\n",
    "|---|---|---|\n",
    "|developer messages are instructions provided by the application developer, prioritized ahead of user messages.|user messages are instructions provided by an end user, prioritized behind developer messages.|Messages generated by the model have the assistant role.|\n",
    "\n",
    "A multi-turn conversation may consist of several messages of these types, along with other content types provided by both you and the model. Learn more about [managing conversation state here](/docs/guides/conversation-state).\n",
    "\n",
    "You could think about `developer` and `user` messages like a function and its arguments in a programming language.\n",
    "\n",
    "*   `developer` messages provide the system's rules and business logic, like a function definition.\n",
    "*   `user` messages provide inputs and configuration to which the `developer` message instructions are applied, like arguments to a function.\n",
    "\n",
    "Message formatting with Markdown and XML\n",
    "----------------------------------------\n",
    "\n",
    "When writing `developer` and `user` messages, you can help the model understand logical boundaries of your prompt and context data using a combination of [Markdown](https://commonmark.org/help/) formatting and [XML tags](https://www.w3.org/TR/xml/).\n",
    "\n",
    "Markdown headers and lists can be helpful to mark distinct sections of a prompt, and to communicate hierarchy to the model. They can also potentially make your prompts more readable during development. XML tags can help delineate where one piece of content (like a supporting document used for reference) begins and ends. XML attributes can also be used to define metadata about content in the prompt that can be referenced by your instructions.\n",
    "\n",
    "In general, a developer message will contain the following sections, usually in this order (though the exact optimal content and order may vary by which model you are using):\n",
    "\n",
    "*   **Identity:** Describe the purpose, communication style, and high-level goals of the assistant.\n",
    "*   **Instructions:** Provide guidance to the model on how to generate the response you want. What rules should it follow? What should the model do, and what should the model never do? This section could contain many subsections as relevant for your use case, like how the model should [call custom functions](/docs/guides/function-calling).\n",
    "*   **Examples:** Provide examples of possible inputs, along with the desired output from the model.\n",
    "*   **Context:** Give the model any additional information it might need to generate a response, like private/proprietary data outside its training data, or any other data you know will be particularly relevant. This content is usually best positioned near the end of your prompt, as you may include different context for different generation requests.\n",
    "\n",
    "Below is an example of using Markdown and XML tags to construct a `developer` message with distinct sections and supporting examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377bb459",
   "metadata": {},
   "source": [
    "```text\n",
    "# Identity\n",
    "\n",
    "You are coding assistant that helps enforce the use of snake case \n",
    "variables in JavaScript code, and writing code that will run in \n",
    "Internet Explorer version 6.\n",
    "\n",
    "# Instructions\n",
    "\n",
    "* When defining variables, use snake case names (e.g. my_variable) \n",
    "  instead of camel case names (e.g. myVariable).\n",
    "* To support old browsers, declare variables using the older \n",
    "  \"var\" keyword.\n",
    "* Do not give responses with Markdown formatting, just return \n",
    "  the code as requested.\n",
    "\n",
    "# Examples\n",
    "\n",
    "<user_query>\n",
    "How do I declare a string variable for a first name?\n",
    "</user_query>\n",
    "\n",
    "<assistant_response>\n",
    "var first_name = \"Anna\";\n",
    "</assistant_response>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6829e392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To declare a variable for a last name in different programming languages, you typically follow the syntax of that language. Here are examples in several common languages:\n",
      "\n",
      "### Python\n",
      "```python\n",
      "last_name = \"Smith\"\n",
      "```\n",
      "\n",
      "### JavaScript\n",
      "```javascript\n",
      "let lastName = \"Smith\";\n",
      "```\n",
      "\n",
      "### Java\n",
      "```java\n",
      "String lastName = \"Smith\";\n",
      "```\n",
      "\n",
      "### C#\n",
      "```csharp\n",
      "string lastName = \"Smith\";\n",
      "```\n",
      "\n",
      "### PHP\n",
      "```php\n",
      "$lastName = \"Smith\";\n",
      "```\n",
      "\n",
      "### Ruby\n",
      "```ruby\n",
      "last_name = \"Smith\"\n",
      "```\n",
      "\n",
      "### C++\n",
      "```cpp\n",
      "std::string lastName = \"Smith\";\n",
      "```\n",
      "\n",
      "### Swift\n",
      "```swift\n",
      "var lastName = \"Smith\"\n",
      "```\n",
      "\n",
      "In each case, replace `\"Smith\"` with the actual last name you want to store.\n"
     ]
    }
   ],
   "source": [
    "# Send a prompt to generate code through the API\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "with open(\"sample_developer_message.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    instructions = f.read()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    instructions=instructions,\n",
    "    input=\"How would I declare a variable for a last name?\",\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3593541e",
   "metadata": {},
   "source": [
    "#### Save on cost and latency with prompt caching\n",
    "\n",
    "When constructing a message, you should try and keep content that you expect to use over and over in your API requests at the beginning of your prompt, **and** among the first API parameters you pass in the JSON request body to [Chat Completions](/docs/api-reference/chat) or [Responses](/docs/api-reference/responses). This enables you to maximize cost and latency savings from [prompt caching](/docs/guides/prompt-caching).\n",
    "\n",
    "Few-shot learning\n",
    "-----------------\n",
    "\n",
    "Few-shot learning lets you steer a large language model toward a new task by including a handful of input/output examples in the prompt, rather than [fine-tuning](/docs/guides/fine-tuning) the model. The model implicitly \"picks up\" the pattern from those examples and applies it to a prompt. When providing examples, try to show a diverse range of possible inputs with the desired outputs.\n",
    "\n",
    "Typically, you will provide examples as part of a `developer` message in your API request. Here's an example `developer` message containing examples that show a model how to classify positive or negative customer service reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f367203",
   "metadata": {},
   "source": [
    "```text\n",
    "# Identity\n",
    "\n",
    "You are a helpful assistant that labels short product reviews as \n",
    "Positive, Negative, or Neutral.\n",
    "\n",
    "# Instructions\n",
    "\n",
    "* Only output a single word in your response with no additional formatting\n",
    "  or commentary.\n",
    "* Your response should only be one of the words \"Positive\", \"Negative\", or\n",
    "  \"Neutral\" depending on the sentiment of the product review you are given.\n",
    "\n",
    "# Examples\n",
    "\n",
    "<product_review id=\"example-1\">\n",
    "I absolutely love this headphones ‚Äî sound quality is amazing!\n",
    "</product_review>\n",
    "\n",
    "<assistant_response id=\"example-1\">\n",
    "Positive\n",
    "</assistant_response>\n",
    "\n",
    "<product_review id=\"example-2\">\n",
    "Battery life is okay, but the ear pads feel cheap.\n",
    "</product_review>\n",
    "\n",
    "<assistant_response id=\"example-2\">\n",
    "Neutral\n",
    "</assistant_response>\n",
    "\n",
    "<product_review id=\"example-3\">\n",
    "Terrible customer service, I'll never buy from them again.\n",
    "</product_review>\n",
    "\n",
    "<assistant_response id=\"example-3\">\n",
    "Negative\n",
    "</assistant_response>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040e82f8",
   "metadata": {},
   "source": [
    "Include relevant context information\n",
    "------------------------------------\n",
    "\n",
    "It is often useful to include additional context information the model can use to generate a response within the prompt you give the model. There are a few common reasons why you might do this:\n",
    "\n",
    "*   To give the model access to proprietary data, or any other data outside the data set the model was trained on.\n",
    "*   To constrain the model's response to a specific set of resources that you have determined will be most beneficial.\n",
    "\n",
    "The technique of adding additional relevant context to the model generation request is sometimes called **retrieval-augmented generation (RAG)**. You can add additional context to the prompt in many different ways, from querying a vector database and including the text you get back into a prompt, or by using OpenAI's built-in [file search tool](/docs/guides/tools-file-search) to generate content based on uploaded documents.\n",
    "\n",
    "#### Planning for the context window\n",
    "\n",
    "Models can only handle so much data within the context they consider during a generation request. This memory limit is called a **context window**, which is defined in terms of [tokens](https://blogs.nvidia.com/blog/ai-tokens-explained) (chunks of data you pass in, from text to images).\n",
    "\n",
    "Models have different context window sizes from the low 100k range up to one million tokens for newer GPT-4.1 models. [Refer to the model docs](/docs/models) for specific context window sizes per model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
